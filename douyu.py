import requests
from bs4 import BeautifulSoup
import os
from collections import defaultdict
import time
import threading
import logging

# é…ç½®æ—¥å¿—ç³»ç»Ÿï¼Œè®°å½•çˆ¬è™«è¿è¡Œä¿¡æ¯
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å®šä¹‰æµè§ˆå™¨ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨å‘é€è¯·æ±‚
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'

# åˆ›å»ºç½‘ç»œè¯·æ±‚ä¼šè¯å¯¹è±¡ï¼Œç”¨äºå‘é€è¯·æ±‚
session = requests.Session()
session.headers.update({'User-Agent': USER_AGENT})

# åˆå§‹åŒ–å¾…çˆ¬å–çš„URLåˆ—è¡¨ï¼Œæ¯ä¸ªURLåŒ…å«å¯¹åº”çš„ç±»åˆ«ä¿¡æ¯
urls = [
    {"url": "https://www.douyu.com/directory/subCate/yqk/290", "category": "é™ªçœ‹"},
    {"url": "https://www.douyu.com/directory/subCate/yqk/1863", "category": "ç»¼åˆ"},
    {"url": "https://www.douyu.com/directory/subCate/yqk/2827", "category": "å–œå‰§"},
    {"url": "https://www.douyu.com/directory/subCate/yqk/2828", "category": "åŠ¨ä½œ"},
    {"url": "https://www.douyu.com/directory/subCate/yqk/2830", "category": "ç§‘å¹»"},
    {"url": "https://www.douyu.com/directory/subCate/yqk/2833", "category": "å‰§æƒ…"},
    {"url": "https://www.douyu.com/directory/subCate/yqk/2834", "category": "å¤è£…"},
    # ...å…¶ä»–URL...
]

# ä½¿ç”¨defaultdictå­˜å‚¨çˆ¬å–åˆ°çš„æ•°æ®ï¼Œä»¥ç±»åˆ«ä¸ºé”®ï¼Œä¸»æ’­ä¿¡æ¯ä¸ºå€¼
all_data = defaultdict(list)
# ä½¿ç”¨é”æ¥å¤„ç†å¤šçº¿ç¨‹é—´çš„å¹¶å‘å†™å…¥é—®é¢˜
lock = threading.Lock()

def crawl_and_parse(url, category):
    """
    çˆ¬å–å¹¶è§£ææŒ‡å®šURLçš„é¡µé¢ï¼Œæå–ä¸»æ’­ä¿¡æ¯ã€‚
    
    :param url: éœ€è¦çˆ¬å–çš„é¡µé¢URL
    :param category: é¡µé¢æ‰€å±çš„ç±»åˆ«
    """
    logging.info(f"å¼€å§‹çˆ¬å–é¡µé¢: {url}")
    try:
        response = session.get(url, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        logging.warning(f"è¯·æ±‚é”™è¯¯: {e}, é¡µé¢: {url}")
        return

    try:
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all(class_="DyListCover-wrap")
        # æå–é¡µé¢ä¸»æ’­ä¿¡æ¯
        page_data = [(link.find(class_="DyListCover-intro")['title'], link['href']) for link in links]
        with lock:
            all_data[category].extend(page_data)
        logging.info(f"æˆåŠŸçˆ¬å–é¡µé¢: {url}, æ”¶å½•{len(page_data)}ä¸ªä¸»æ’­")
    except Exception as e:
        logging.error(f"è§£æé”™è¯¯: {e}, é¡µé¢: {url}")

def threaded_crawler():
    """
    ä½¿ç”¨å¤šçº¿ç¨‹çˆ¬å–æ‰€æœ‰URLåˆ—è¡¨ä¸­çš„é¡µé¢ã€‚
    """
    threads = []
    for url_info in urls:
        t = threading.Thread(target=crawl_and_parse, args=(url_info["url"], url_info["category"]))
        threads.append(t)
        t.start()
        time.sleep(1)  # æ§åˆ¶çº¿ç¨‹å¯åŠ¨é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé›†ä¸­

    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in threads:
        t.join()

def write_to_current_directory(filename='douyu.txt'):
    """
    å°†çˆ¬å–åˆ°çš„æ•°æ®å†™å…¥å½“å‰ç›®å½•ä¸‹çš„æ–‡æœ¬æ–‡ä»¶ä¸­ã€‚

    :param filename: è¾“å‡ºæ–‡ä»¶åï¼Œé»˜è®¤ä¸º'douyu.txt'
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    full_path = os.path.join(script_dir, filename)

    # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶ï¼Œå†™å…¥æ ‡é¢˜è¡Œ
    with open(full_path, 'w', encoding='utf-8') as file:
        file.write("ğŸ æ–—é±¼ç›´æ’­ğŸ ,#genre#\n")

    # è¿½åŠ æ•°æ®åˆ°æ–‡ä»¶
    with open(full_path, 'a', encoding='utf-8') as file:
        for category, data in all_data.items():
            for title, url in data:
                # æ ¼å¼åŒ–è¾“å‡ºæ•°æ®
                modified_url = url.replace("/", "")
                content = f"{title},http://192.168.1.43:81/douyu.php?id={modified_url}"
                file.write(f"{content}\n")
        file_position = file.tell()  # è·å–æ–‡ä»¶å†™å…¥ä½ç½®
    logging.info(f"æ•°æ®å·²å†™å…¥æ–‡ä»¶: {full_path}, æœ€åå†™å…¥ä½ç½®: {file_position}å­—èŠ‚")
    if os.path.exists(full_path) and os.path.getsize(full_path) > 0:
        logging.info("æ–‡ä»¶å†™å…¥æˆåŠŸä¸”éç©ºã€‚")
    else:
        logging.warning("æ–‡ä»¶å¯èƒ½æœªè¢«å†™å…¥æˆ–ä¸ºç©ºã€‚")

if __name__ == "__main__":
    logging.info("å¼€å§‹çˆ¬å–æ–—é±¼ä¸»æ’­ä¿¡æ¯...")
    threaded_crawler()
    logging.info("æ‰€æœ‰çˆ¬å–ä»»åŠ¡å®Œæˆï¼Œå¼€å§‹å†™å…¥æ•°æ®...")
    write_to_current_directory()
    logging.info("æ‰€æœ‰æ“ä½œå®Œæˆã€‚")